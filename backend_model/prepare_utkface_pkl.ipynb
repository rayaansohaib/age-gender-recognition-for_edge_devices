{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9z8rCBk9AaB",
        "outputId": "586013ba-812b-456f-9575-86237324c2f8"
      },
      "id": "a9z8rCBk9AaB",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Copy the zip (via the shortcut) into local storage\n",
        "!cp \"/content/drive/MyDrive/UTKface.zip\" /content/UTKface.zip\n",
        "\n",
        "# 3️⃣ Unzip it to /content/UTKface\n",
        "!unzip -q /content/UTKface.zip -d /content/UTKface\n"
      ],
      "metadata": {
        "id": "HgiiWKy5_KfI"
      },
      "id": "HgiiWKy5_KfI",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dcf4af17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcf4af17",
        "outputId": "bee895da-73b2-45c5-9b3e-b7bddeaeaf3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prepare_utkface_pkl.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile prepare_utkface_pkl.py\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def parse_filename(filename):\n",
        "    parts = filename.split('_')\n",
        "    age = int(parts[0])\n",
        "    gender = int(parts[1])\n",
        "    ethnicity = int(parts[2])\n",
        "    return age, gender, ethnicity\n",
        "\n",
        "\n",
        "def load_and_process(path, size):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = img.resize((size, size), Image.BILINEAR)\n",
        "    arr = np.array(img, dtype=np.uint8)\n",
        "    tensor = torch.from_numpy(arr).permute(2, 0, 1).float().div(255.0)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def main(input_dir: str, output_dir: str, size: int = 224, chunks: int = 24):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    # Collect all image file paths\n",
        "    file_paths = []\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for fname in files:\n",
        "            if fname.lower().endswith(('.jpg', '.png')):\n",
        "                file_paths.append(os.path.join(root, fname))\n",
        "    print(f\"Found {len(file_paths)} images under {input_dir}\")\n",
        "\n",
        "    # Shuffle paths\n",
        "    random.seed(42)\n",
        "    random.shuffle(file_paths)\n",
        "    total = len(file_paths)\n",
        "    chunk_size = total // chunks\n",
        "\n",
        "    # Process in chunks to limit RAM usage\n",
        "    for idx in range(chunks):\n",
        "        start = idx * chunk_size\n",
        "        end = (idx + 1) * chunk_size if idx < chunks - 1 else total\n",
        "        chunk_paths = file_paths[start:end]\n",
        "\n",
        "        images, ages, genders, ethnics = [], [], [], []\n",
        "        for path in tqdm(chunk_paths, desc=f\"Chunk {idx}\"):\n",
        "            name = os.path.basename(path)\n",
        "            try:\n",
        "                age, gender, ethnicity = parse_filename(name)\n",
        "            except:\n",
        "                continue\n",
        "            tensor = load_and_process(path, size)\n",
        "            images.append(tensor)\n",
        "            ages.append(age)\n",
        "            genders.append(gender)\n",
        "            ethnics.append(ethnicity)\n",
        "\n",
        "        out_path = os.path.join(output_dir, f\"{idx}.pkl\")\n",
        "        with open(out_path, 'wb') as f:\n",
        "            pickle.dump({'images': images,\n",
        "                         'ages': ages,\n",
        "                         'genders': genders,\n",
        "                         'ethnicities': ethnics}, f)\n",
        "        print(f\"Saved chunk {idx} with {len(images)} samples to {out_path}\")\n",
        "        # free memory\n",
        "        del images, ages, genders, ethnics\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_dir', type=str, required=True,\n",
        "                        help='Root UTKFace folder to recurse')\n",
        "    parser.add_argument('--output_dir', type=str, required=True,\n",
        "                        help='Directory to write PKL chunks')\n",
        "    parser.add_argument('--size', type=int, default=224,\n",
        "                        help='Image resize dimension')\n",
        "    parser.add_argument('--chunks', type=int, default=24,\n",
        "                        help='Number of chunks to split into')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input_dir, args.output_dir, args.size, args.chunks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0bad3796",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bad3796",
        "outputId": "53085ec6-ebe5-424a-8841-60a8584ebf0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 24068 images under /content/UTKface\n",
            "Chunk 0: 100% 1002/1002 [00:12<00:00, 82.44it/s]\n",
            "Saved chunk 0 with 1002 samples to /content/drive/MyDrive/pkl_files/0.pkl\n",
            "Chunk 1: 100% 1002/1002 [00:11<00:00, 88.08it/s]\n",
            "Saved chunk 1 with 1002 samples to /content/drive/MyDrive/pkl_files/1.pkl\n",
            "Chunk 2: 100% 1002/1002 [00:15<00:00, 66.53it/s]\n",
            "Saved chunk 2 with 1002 samples to /content/drive/MyDrive/pkl_files/2.pkl\n",
            "Chunk 3: 100% 1002/1002 [00:11<00:00, 87.15it/s]\n",
            "Saved chunk 3 with 1002 samples to /content/drive/MyDrive/pkl_files/3.pkl\n",
            "Chunk 4: 100% 1002/1002 [00:13<00:00, 74.82it/s]\n",
            "Saved chunk 4 with 1002 samples to /content/drive/MyDrive/pkl_files/4.pkl\n",
            "Chunk 5: 100% 1002/1002 [00:15<00:00, 65.80it/s]\n",
            "Saved chunk 5 with 1002 samples to /content/drive/MyDrive/pkl_files/5.pkl\n",
            "Chunk 6: 100% 1002/1002 [00:13<00:00, 72.64it/s]\n",
            "Saved chunk 6 with 1002 samples to /content/drive/MyDrive/pkl_files/6.pkl\n",
            "Chunk 7: 100% 1002/1002 [00:13<00:00, 73.95it/s]\n",
            "Saved chunk 7 with 1002 samples to /content/drive/MyDrive/pkl_files/7.pkl\n",
            "Chunk 8: 100% 1002/1002 [00:10<00:00, 99.41it/s]\n",
            "Saved chunk 8 with 1002 samples to /content/drive/MyDrive/pkl_files/8.pkl\n",
            "Chunk 9: 100% 1002/1002 [00:10<00:00, 96.67it/s]\n",
            "Saved chunk 9 with 1002 samples to /content/drive/MyDrive/pkl_files/9.pkl\n",
            "Chunk 10: 100% 1002/1002 [00:10<00:00, 98.26it/s]\n",
            "Saved chunk 10 with 1002 samples to /content/drive/MyDrive/pkl_files/10.pkl\n",
            "Chunk 11: 100% 1002/1002 [00:10<00:00, 94.67it/s]\n",
            "Saved chunk 11 with 1002 samples to /content/drive/MyDrive/pkl_files/11.pkl\n",
            "Chunk 12: 100% 1002/1002 [00:10<00:00, 96.76it/s]\n",
            "Saved chunk 12 with 1001 samples to /content/drive/MyDrive/pkl_files/12.pkl\n",
            "Chunk 13: 100% 1002/1002 [00:10<00:00, 94.92it/s]\n",
            "Saved chunk 13 with 1002 samples to /content/drive/MyDrive/pkl_files/13.pkl\n",
            "Chunk 14: 100% 1002/1002 [00:10<00:00, 99.26it/s]\n",
            "Saved chunk 14 with 1002 samples to /content/drive/MyDrive/pkl_files/14.pkl\n",
            "Chunk 15: 100% 1002/1002 [00:09<00:00, 101.13it/s]\n",
            "Saved chunk 15 with 1001 samples to /content/drive/MyDrive/pkl_files/15.pkl\n",
            "Chunk 16: 100% 1002/1002 [00:11<00:00, 90.14it/s]\n",
            "Saved chunk 16 with 1002 samples to /content/drive/MyDrive/pkl_files/16.pkl\n",
            "Chunk 17: 100% 1002/1002 [00:09<00:00, 100.97it/s]\n",
            "Saved chunk 17 with 1002 samples to /content/drive/MyDrive/pkl_files/17.pkl\n",
            "Chunk 18: 100% 1002/1002 [00:09<00:00, 103.80it/s]\n",
            "Saved chunk 18 with 1002 samples to /content/drive/MyDrive/pkl_files/18.pkl\n",
            "Chunk 19: 100% 1002/1002 [00:09<00:00, 102.17it/s]\n",
            "Saved chunk 19 with 1002 samples to /content/drive/MyDrive/pkl_files/19.pkl\n",
            "Chunk 20: 100% 1002/1002 [00:11<00:00, 91.05it/s]\n",
            "Saved chunk 20 with 1002 samples to /content/drive/MyDrive/pkl_files/20.pkl\n",
            "Chunk 21: 100% 1002/1002 [00:10<00:00, 95.16it/s]\n",
            "Saved chunk 21 with 1002 samples to /content/drive/MyDrive/pkl_files/21.pkl\n",
            "Chunk 22: 100% 1002/1002 [00:09<00:00, 104.39it/s]\n",
            "Saved chunk 22 with 1002 samples to /content/drive/MyDrive/pkl_files/22.pkl\n",
            "Chunk 23: 100% 1022/1022 [00:11<00:00, 92.45it/s]\n",
            "Saved chunk 23 with 1022 samples to /content/drive/MyDrive/pkl_files/23.pkl\n"
          ]
        }
      ],
      "source": [
        "# 4️⃣ Run the preprocessing script on the newly unzipped folder\n",
        "!python prepare_utkface_pkl.py \\\n",
        "    --input_dir \"/content/UTKface\" \\\n",
        "    --output_dir \"/content/drive/MyDrive/pkl_files\" \\\n",
        "    --size 224 \\\n",
        "    --chunks 24\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pkl file names were changed from num.pkl (5.pkl) to chunk_numnum.pkl (chunk_05.pkl) at some point of our work"
      ],
      "metadata": {
        "id": "IdC3JQxdFggy"
      },
      "id": "IdC3JQxdFggy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}